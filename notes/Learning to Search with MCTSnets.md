## [Learning to Search with MCTSnets](https://arxiv.org/abs/1802.04697)
#### Arthur Guez, Théophane Weber, Ioannis Antonoglou, Karen Simonyan, Oriol Vinyals, Daan Wierstra, Rémi Munos, David Silver (2018)

**Short Summary**: Guez et. al. replace all key subroutines of MCTS (expansion, evaluation, simulation, backup, readout) with learnable components to try to learn to search. They train the full residual-recursive network with variants of the REINFORCE rule in a supervised learning setup by minimizing the cross-entropy between the final action probabilties read out by the network and some target distribution. They evaluate on 10x10 Sokoban and find good performance!

My favorite parts are Sections 3.2 and 3.3: because the novelty of the paper is mostly in the architecture itself, they detail it and its forward-mode use in two different ways, one as a line-for-line comparison to a typical MCTS algorithm, and another as a recursive memory-based neural network that updates a large embedding vector with different sub-networks. 

Justifiably, care must be taken when training this network since a large number of simulations leads to a growing set of node embeddings to compute and a growing number of `simulation()` calls that need to be made. They first formulate a simple REINFORCE-like objective but then derive a lower-variance anytime algorithm that not only uses the terminal loss as the reinforcement signal, but the difference between the terminal loss and the loss computed before a particular step m started. They also introduce a discounting factor in the objective (likely *quite* important) to further lower the variance and incentivize shorter term improvements, but introduce bias. They also use a particular residual, gating mechanism in the backup network. 

In the experiments, they outperform some baseline model-free and model-based agents on Sokoban and reach ~84% of all levesl(!) (but in the supervised setting). One cool experiment shows taht indeed, MCTSnet extracts info contained in states visited during the search! In my eyes, this is a fantastic step towards learning to reuse the computation by MCTS. Even with a simple random simulation policy, MCTSnet performance improves on it. 

The paper is one of the few I know of to explore the space of learning to search, especialy with an eye towards applying this to RL. Like AlphaGo and following work, MCTSnets relies on a provided model of the environment, but can easily be extended to using a learned model (e.g. MuZero!) and could be used in RL by using the MCTS step as policy iteration. A version was rejected from ICLR for not having other experiments outside Sokoban, but was later accepted at ICML. There's a lot of room for improvement here, but I imagine some of it is hampered by the implementation complexity of the model. It feels like there should be simpler approaches to learning to search, but I suppose that's what future work is for! 
